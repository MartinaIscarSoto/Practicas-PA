---
# this is the Rmarkdown header. Use the YAML syntax. ---YAML block ---.
title: "Employee Engagement" 
subtitle: "Groupwork Use Case 2 - People Analytics"
author: "Ferrán Extremera, Irene Morales, Amalia Varo y Martina Iscar Substitute Group_X by your Group number in the name of the .Rmd file"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide

# the following lines tell R to store the output in the subdirectory reports
knit: (function(inputFile, encoding) { 
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_dir=file.path(dirname(inputFile),"report" )) }) 
---

# Intro

In this workshop, you will analyze employee engagement using both individual- and team-level data. You’ll apply factor analysis, reliability testing, t-tests, and multiple regression to real-world employee survey data. The goal is to understand how engagement is measured, how well survey instruments perform, and what factors predict engagement across teams.

# Setup

Within the project directory, create the following folders if they do not already exist:

-   **Databases**: to store all data sources provided in Moodle (Excel files).\
-   **data**: to store any ad-hoc data created during the analysis. The native R data format is .rds.\
-   **report**: to store the HTML output file generated when knitting the R Markdown file.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) 
library(readxl)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(readxl, psych, dplyr, GPArotation, scatterplot3d, modelsummary, car, this.path, huxtable, lm.beta)
```

# Prompt 1: Exploratory Factor Analysis on Engagement Items

### Tasks

-   Import 'Ch5_RAW_Survey_Results.xlsx' into R (definitions of all fields are included in the 'variable key' worksheet within the Excel file).\
-   Select the seven engagement and support items (Eng1–Eng4, pos1–pos3).\
-   Conduct an exploratory factor analysis with parallel analysis and oblimin rotation.\
-   Interpret the factor structure: how many latent constructs are present?\
-   Assign names to the resulting factors based on item content.

### Deliverables

-   Factor loadings table and scree plot.\
-   Interpretation of number and type of constructs.\
-   Short reflection: How could these constructs guide HR intervention design?

## Solution

```{r}
df <- read_excel("Databases/Ch5_RAW_Survey_Results.xlsx")
head(df)
```

Now we proceed to filter to select the engagement and support items.

```{r}
data<- df %>% 
  select(Eng1,Eng2,Eng3,Eng4,pos1,pos2, pos3)

head(data)

```

Once the variables are selected, we conduct an exploratory factor analysis with parallel analysis and oblimin rotation. We start by doing two tests that work as the "gatekeepers" of the analysis. Before trying to group variables into factors, we need to prove that they actually relate to each other in a meaningful way. If your variables aren't correlated, the results would just be statistical noise.

### Factor loadings table and scree plot

```{r}
# Let's assume your dataframe is named 'data'
# Check suitability with KMO and Bartlett's Test
KMO(data)
cortest.bartlett(data)
```

**1. KMO (Kaiser-Meyer-Olkin)** TestYour Overall MSA = 0.79 is very strong.
  Interpretation: On the scale of sampling adequacy, 0.79 is considered "Good" (bordering on "Meritorious"). This value confirms      that the variance in the variables is likely caused by underlying factors. It is well above the minimum threshold   of 0.50.
  Item Breakdown: Eng1 through pos1 are performing exceptionally well (\> 0.80). While pos2 and pos3 are slightly lower at 0.64,      they are still well within the acceptable range for factor analysis.

**2.  Bartlett’s Test of Sphericity**: the results shows a \$p.value = 0 (which means $p < .0001$).
  Interpretation: Since the p-value is less than 0.05, the result is statistically significant. This rejects the null   hypothesis    that our variables are unrelated. It proves there is enough correlation in the data to justify running     an EFA.

```{r}
# Run Parallel Analysis
set.seed(1615)
parallel_res <- fa.parallel(data, fm = "minres", fa = "fa")
```
The Parallel Analysis Scree Plot determines the number of factors to extract:

The Decision Rule is to retain factors where the blue line (Actual Data) is above the red dotted line (Simulated Data).

As seen in the plot, the blue triangles for Factor 1 and Factor 2 are clearly above the red lines. Factor 3 appears to sit almost exactly on the line or slightly above it.

Looking at the steep drop after the second factor, a 2-factor solution is the most robust choice for your EFA.´But sometimes the model may give a 3-factor solution, therefore we set a seed

### Interpretation of number and type of constructs
```{r}
# fm = "minres" is a common default; "ml" (Maximum Likelihood) is also popular
# rotate = "oblimin" for oblique rotation
efa_model <- fa(data, nfactors = 3, rotate = "oblimin", fm = "minres")

# View the results
print(efa_model$loadings, cutoff = 0.3) # We hide loadings below 0.3 for clarity
```
Even though the more common result is a 2 factor test we evaluate the model with 3-factor and compare it later to the 2-factor one.

The factor structure analysis reveals a clear distinction between the dimensions evaluated. The first factor (MR1) is extremely robust and represents the "Eng" dimension, with very high factor loadings ranging from 0.807 to 0.910 for items Eng1 to Eng4. The second factor (MR2) captures the "pos" dimension, where the loading of pos3 stands out at 0.916, while pos1 presents the most moderate loading of the group at 0.471. Conversely, the third factor (MR3) lacks a clear structure, as only Eng3 presents a marginal loading of 0.300 and its eigenvalue (SS loading) is a mere 0.188, which is extremely low and suggests that this factor is not relevant to the model.

To illustrate it we show the diagram, but it presents an error because it does not assign Eng3 to the MR3, however understanding the table above we know that the proportion is really low and this diagram is not accurate and we will not work with the 3-factor model.

```{r}
fa.diagram(efa_model)
```
Summary of conclusions of 3-factor selection:
- Discard Factor 3: The third factor (MR3) explains only 2.7% of the variance and has no strong unique loadings.
- Clean Structure: Your variables group perfectly by their naming convention (Eng vs. pos), showing excellent convergent and          discriminant validity.
- High Explained Variance: Together, the first two factors (MR1 and MR2) explain 65.3% of the total variance in your data, which is   a very respectable result for social science research.

After realizing the 3-factor model should be discarded, we do the 2-factor EFA:

```{r}
final_efa <- psych::fa(data, 
                       nfactors = 2, 
                       rotate = "oblimin", 
                       fm = "minres")

# Display the factor loadings
# This reveals the 2 latent constructs: MR1 (Eng variables) and MR2 (pos variables)
print(final_efa$loadings, cutoff = 0.3)
```
```{r}
# Check Factor Correlations
# Since we used Oblimin, this shows how the two latent constructs relate to each other
final_efa$Phi
```
```{r}
# Visualize the latent structure
# This provides a clear diagram of the two identified constructs
psych::fa.diagram(final_efa)
```
### Short reflection: How could these constructs guide HR intervention design?
Looking at the correlation matrix between MR1 and MR2 (second image), the correlation is 0.258.

Meaning: This is a low-to-moderate positive correlation.

Interpretation: Since the correlation is relatively low (typically below 0.30 or 0.40), it suggests that while these two factors are related, they are distinct enough to be considered separate constructs. You likely used an oblique rotation (like Promax or Oblimin) to allow them to correlate; if they were completely independent, this value would be 0.
EFA Summary: Loadings and Variance
The Loadings table (third image) shows how each item "belongs" to a factor.

Simple Structure: You have achieved a "simple structure," which is the gold standard in EFA. Every item loads strongly onto one factor (all > 0.40) and has no "cross-loadings" (loading onto both factors at the same time).

Factor 1 (MR1): Includes Eng1, Eng2, Eng3, and Eng4. These loadings are very high (0.78 to 0.87), indicating these items are excellent indicators of the MR1 construct.

Factor 2 (MR2): Includes pos1, pos2, and pos3. While pos1 is a bit lower (0.47), pos3 is a powerhouse at 0.92.

Variance Explained: * MR1 explains 40.6% of the total variance.

MR2 explains 24.3% of the total variance.

Cumulative: Together, they explain 64.9% of the variance in your data, which is well above the common 50-60% threshold for a "good" model in social sciences.
In EFA, "Latent Constructs" are the unobserved "hidden" variables (MR1 and MR2) that cause people to answer the questions the way they do.

MR1 (The "Eng" Factor): Based on the labels, this likely represents "Engagement" or "English Proficiency" (depending on your study). The high loadings suggest these four items are very consistent in measuring this single idea.

MR2 (The "Pos" Factor): This likely represents a "Positive Outlook" or "Position" construct.

The Parallel Analysis (Fourth Image): This plot confirms that a 2-factor solution was the correct choice. The blue line (actual data) drops below the red dashed line (simulated/random data) after the second factor, meaning any factor beyond the second is just "noise."

Your analysis tells a very clear story: Your survey effectively measures two distinct things. Because the KMO was strong and the Bartlett's test was significant, we can trust that these patterns aren't just a fluke in the numbers. The high "Cumulative Var" (64.9%) means that by using just these two factors, you are capturing the vast majority of the information contained in all 7 individual items. This simplifies your research significantly—instead of analyzing 7 different variables, you can now analyze just 2 "Super-Variables" (Factor Scores).

Factor 1 (MR1): Organizational Identification & Advocacy
The items in this factor (Eng1 through Eng4) go beyond simple "engagement." They measure how much an employee incorporates the company into their own identity.

Eng1 & Eng2: Values and Pride.

Eng3: Identity ("...a big part of who I am").

Eng4: Advocacy ("I would recommend...").

Interpretation: This factor represents Internalized Loyalty. High scores here mean employees aren't just working for a paycheck; they believe in the "brand" of the organization.

Factor 2 (MR2): Perceived Organizational Support (POS)
The "pos" items (pos1 through pos3) clearly measure the employees' perception of how much the organization values them and cares about their welfare.

pos1: Well-being.

pos2: Contribution value.

pos3: Support/Help.

Interpretation: This factor represents the Safety Net. It is the "give-and-take" relationship. Since pos3 ("Organization is there for me when I need help") has the highest loading (0.919), the core of this factor is Reliability in Crisis.

2. HR Intervention Design
Strategy for MR1: Strengthening the "Bond"
Since MR1 explains the most variance (40.6%), your primary HR goal should be culture-building.

Values Alignment (Eng1): Re-evaluate the onboarding process to ensure company values are not just stated, but demonstrated through stories and leadership behavior.

Brand Ambassadorship (Eng4): Because this is your highest loading item for this factor (0.877), create "Employee Referral" programs or "Culture Committees" where these advocates can lead internal initiatives.

Strategy for MR2: Enhancing the "Safety Net"
Because this factor is distinct (correlation only 0.258), it requires a different approach focused on empathy and resources.

Support Systems (pos3): This is your "Power Item" (loading 0.919). HR should ensure that "Help" is visible and accessible—such as Employee Assistance Programs (EAPs), clear mental health days, or "Manager Open Door" policies.

Recognition of Contribution (pos2): With a strong loading of 0.793, interventions should focus on recognition programs that specifically highlight how an individual's work contributed to a success, rather than just generic "good job" emails.

3. Summary of Meaning
The low correlation (0.258) between these factors is a vital insight for HR. It means that an employee can love the company’s values (High MR1) but feel like the company doesn't care about them personally (Low MR2).

If you only focus on "Culture" (MR1), you risk losing people to burnout because they don't feel supported. If you only focus on "Support" (MR2), you might have happy employees who don't actually care about the company's success or mission.

# Prompt 2: Scale Reliability Testing

### Tasks

-   Import 'Ch5_RAW_Survey_Results.xlsx' into R (definitions of all fields are included in the 'variable key' worksheet within the Excel file).\
-   Select the four OCB items (ocb1–ocb4).\
-   Compute Cronbach’s alpha for the scale.\
-   Identify any items that reduce reliability and could be dropped.\
-   Reflect on the trade-off between brevity and reliability.

### Deliverables

-   Cronbach’s alpha result with interpretation.\
-   Recommendation on whether to keep or drop specific items.\
-   Reflection: Why does internal consistency matter in employee surveys?

## Solution

### Cronbach’s alpha result with interpretation

```{r prompt2.1}
df2 <- read_excel("Databases/Ch5_RAW_Survey_Results.xlsx")
ocb_items <- df2[, c("ocb1", "ocb2", "ocb3", "ocb4")]

```
These lines of code load the required packages (readxl and psych) needed to import Excel data and perform reliability analysis. The dataset is then imported into R using read_excel() and stored in the object data. Finally, the four OCB items (ocb1–ocb4) are selected from the dataset and stored in a new object called ocb_items, which will be used for further analysis.´

```{r}
alpha_result <- alpha(ocb_items) ##Calcula el Cronbach alpha para esos 4 items
alpha_result ##Imprime el resumen general del analisis de fiabilidad
```

These lines calculate Cronbach’s alpha for the four OCB items and store the results in `alpha_result`. Printing the object displays the overall reliability statistics for the scale.

```{r}
alpha_result$total$raw_alpha ##Extrae el alpha clásico de la escala completa 
alpha_result$alpha.drop ##Muestra la tabla de "alpha if item deleted" en otras palabras, que pasaria con la fiabilidad si eliminas cada item
```
These lines extract the overall Cronbach’s alpha for the full scale and display the “alpha if item deleted” table, which shows how reliability would change if each item were removed.

### Recommendation on whether to keep or drop specific items

### Reflection: Why does internal consistency matter in employee surveys?

# Prompt 3: Principal Component Analysis on Aggregated Survey Items

### Tasks

-   Import 'Ch5_SURVEY_PROVIDER_ENGAGEMENT_DATA_TEAM_LEVEL.xlsx' into R (definitions of all fields are included in the 'variable key' worksheet within the Excel file).\
-   Conduct a principal component analysis (PCA) on the 9 team-level engagement items.\
-   Identify and name the components.\
-   Interpret what each component represents in terms of employee experience.

### Deliverables

-   Component loadings table.\
-   3D scatterplot showing component space (optional).\
-   Description of the constructs found and their strategic relevance.

## Solution

### Component loadings table

```{r prompt3.1}
# Importar el archivo (ajusta el nombre si es necesario)
file_path <- "Databases/Ch5_SURVEY_PROVIDER_ENGAGEMENT_DATA_TEAM_LEVEL.xlsx"
df3 <- read_excel(file_path, sheet = 1)

# Seleccionar los 9 ítems de compromiso (Eng1 a Eng9)
engagement_items <- df3[, c("Eng1", "Eng2", "Eng3", "Eng4", "Eng5", "Eng6", "Eng7", "Eng8", "Eng9")]

# Ejecutar PCA con rotación varimax
# Basado en el análisis, 3 componentes suelen explicar mejor la varianza en este set
pca_result <- principal(engagement_items, nfactors = 3, rotate = "varimax")

# Deliverable: Tabla de cargas de componentes
print(pca_result$loadings, cutoff = 0.4, digits = 3)

```
### 3D scatterplot showing component space (optional)

```{r prompt3.2}
# Extraer las puntuaciones de los componentes por equipo
scores <- as.data.frame(pca_result$scores)

# Deliverable: Scatterplot 3D
scatterplot3d(scores[,1:3], 
              pch = 16, color = "steelblue",
              main = "Espacio de Componentes de Compromiso",
              xlab = "Comp 1: Org. y Propósito", 
              ylab = "Comp 2: Bienestar", 
              zlab = "Comp 3: Ética y Seguridad")

```

### Description of the constructs found and their strategic relevance

# Prompt 4: t-Tests on Engagement Across Teams

### Tasks

-   Import 'Ch5_Engagement_CASE_Group_level_data_2023.xlsx' into R (definitions of all fields are included in the 'variable key' worksheet within the Excel file).\
-   Conduct t-tests comparing engagement by location (London vs Other) and by function (Sales vs Prof. Services).\
-   Use Levene’s test to check for equal variances.\
-   Interpret results and discuss practical implications.

### Deliverables

-   t-test outputs with interpretation.\
-   Summary of significant and non-significant findings.\
-   Recommendations: Should HR act on these differences?

## Solution

### t-test outputs with interpretation

```{r prompt4.1}
# 1. Importar los datos
df4 <- read_excel("Databases/Ch5_Engagement_CASE_Group_level_data_2023.xlsx", sheet = "data")

# 2. Preparar los factores
df4$LondonorNot <- factor(df4$LondonorNot, levels = c(1, 2), labels = c("London", "Other"))
df4$Function <- factor(df4$Function, levels = c(1, 2), labels = c("Sales", "Prof. Services"))

# --- COMPARACIÓN POR UBICACIÓN ---
leveneTest(EMPsurvEngagement ~ LondonorNot, data = df4) # p > 0.05: Varianzas iguales
t_loc <- t.test(EMPsurvEngagement ~ LondonorNot, data = df4, var.equal = TRUE)
print(t_loc)

# --- COMPARACIÓN POR FUNCIÓN ---
leveneTest(EMPsurvEngagement ~ Function, data = df4) # p > 0.05: Varianzas iguales
t_fun <- t.test(EMPsurvEngagement ~ Function, data = df4, var.equal = TRUE)
print(t_fun)

```

### Summary of significant and non-significant findings

```{r prompt4.2}
# Resumen visual rápido
boxplot(EMPsurvEngagement ~ Function, data = df4, 
        main = "Compromiso por Función", col = c("orange", "lightblue"))

```

### Recommendations: Should HR act on these differences?

# Prompt 5: Multiple Regression to Predict Team-Level Engagement

### Tasks

-   Import 'Ch5_Engagement_CASE_Group_level_data_2023.xlsx' into R (definitions of all fields are included in the 'variable key' worksheet within the Excel file).\
-   Run a multiple regression with engagement as the dependent variable. Use predictors: supervisor score, integrity score, team size, % male, % UG, location, and function.\
-   Analyze standardized coefficients and significance levels.\
-   Identify the most important drivers of engagement.

### Deliverables

-   Regression summary table.\
-   Interpretation of model strength and variable impacts.\
-   Actionable recommendations for HR strategy.

## Solution

### Regression summary table

```{r prompt5.1}
# 1. Carga de datos
df5 <- read_excel("Databases/Ch5_Engagement_CASE_Group_level_data_2023.xlsx", sheet = "data")

# 2. Preparar variables (asegurar que sean factores donde corresponda)
df5$Location <- as.factor(df5$Location)
df5$Function <- as.factor(df5$Function)

# 3. Ejecutar el modelo de regresión múltiple
modelo <- lm(EMPsurvEngagement ~ EmpSurvSupervisor + EmployeeSurvOrgIntegrity + 
             GroupSize + PercentMale + UG + Location + Function, data = df5)

# 4. Obtener coeficientes estandarizados
modelo_beta <- lm.beta(modelo)

# Deliverable: Resumen de la regresión
summary(modelo_beta)

```

### Interpretation of model strength and variable impacts

### Actionable recommendations for HR strategy
